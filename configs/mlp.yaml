# MLP-specific overrides
model:
  type: "mlp"
  hidden_layers: [128, 64, 32]
  activation: "relu"
  dropout: 0.2
  learning_rate: 0.001

training:
  epochs: 100
  batch_size: 32

experiment:
  name: "mlp_experiment"