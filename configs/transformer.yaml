# Transformer-specific overrides
model:
  type: "transformer"
  d_model: 128
  nhead: 8
  num_layers: 6
  dim_feedforward: 512
  dropout: 0.1
  learning_rate: 0.0001

training:
  epochs: 200
  batch_size: 8

data:
  sequence_length: 64

experiment:
  name: "transformer_experiment"